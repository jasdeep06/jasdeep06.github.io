<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="CNNs in Tensorflow(cifar-10)">

    <link rel="stylesheet" type="text/css" media="screen" href="blog_style.css">

    <title>Understanding LSTM in Tensorflow</title>
   <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro" rel="stylesheet">
    <script>
      window.location = "https://www.knowledgemapper.com/knowmap/knowbook/jasdeepchhabra94@gmail.comUnderstandingLSTMinTensorflow(MNISTdataset)";
    </script>
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-90643331-1', 'auto');
  ga('send', 'pageview');
</script>
<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
  </head>
  <body style="font-family: 'Source Sans Pro', sans-serif;font-size:20px">
    <div id="header_wrap" class="outer">
        <header class="inner">

          <h1 id="project_title">Tensorflow</h1>
                                  <p style="text-align:right;color:#fff;font-size: 20px;font-weight: 700"><a href="https://jasdeep06.github.io/">HOME</a></p>


          
        </header>
    </div>
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">

<h1 id="understanding-lstm-in-tensorflowmnist-dataset">Understanding LSTM in Tensorflow(MNIST dataset)</h1>

<p>Long Short Term Memory(LSTM) are the most common types of Recurrent Neural Networks used these days.They are mostly used with sequential data.An in depth look at LSTMs can be found in this <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">incredible blog post</a>.</p>



<h2 id="our-aim">Our Aim</h2>

<p>As the title suggests,the main aim of this blogpost is to make the reader comfortable with the implementation details of basic LSTM network in tensorflow.</p>

<p>For fulfilling this aim we will take <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a> as our dataset.</p>



<h2 id="the-mnist-dataset">The MNIST dataset</h2>

<p>The MNIST dataset consists of images of handwritten digits and their corresponding labels.We can download and read the data in tensorflow with the help of following in built functionality-</p>

<pre class="prettyprint"><code>from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("/tmp/data/", one_hot=True)
</code></pre>

<p>The data is split into three parts-</p>

<ol>
<li>Training data(<code>mnist.train</code>)-55000 images of training data</li>
<li>Test data(<code>mnist.test</code>)-10000 images of test data</li>
<li>Validation data(<code>mnist.validation</code>)-5000 images of validation data.</li>
</ol>



<h3 id="shape-of-the-data">Shape of the data</h3>

<p>Let us discuss the shape with respect to training data of MNIST dataset.Shapes of all three splits are identical.</p>

<p>The training set consists of 55000 images of 28 pixels X 28 pixels each.These 784(28X28) pixel values are flattened in form of a single vector of dimensionality 784.The collection of all such 55000 pixel vectors(one for each image) is stored in form of a numpy array of shape <code>(55000,784)</code> and is referred to as <code>mnist.train.images</code>.</p>

<p>Each of these 55000 training images are associated with a label representing the class to which that image belongs.There are 10 such classes(0,1,2…9).Class labels are represented in one hot encoded form.Thus the labels are stored in form of numpy array of shape <code>(55000,10)</code> and is referred to as <code>mnist.train.labels</code>.</p>



<h2 id="why-mnist">Why MNIST?</h2>

<p>LSTMs are generally used for complex sequence related problems like language modelling which involves NLP concepts such as word embeddings, encoders etc.These topics themselves need a lot of understanding.It would be nice to eliminate these topics to concentrate on implementation details of LSTMs in tensorflow such as input formatting,LSTM cells and network designing.</p>

<p>MNIST gives us such an opportunity.The input data here is just a set of pixel values.We can easily format these values and concentrate on implementation details.</p>



<h2 id="implementation">Implementation</h2>

<p>Before getting our hands dirty with code,let us first draw an outline of our implementation.This will make the coding part more intuitive.</p>



<h3 id="a-vanilla-rnn">A vanilla RNN</h3>

<p>A Recurrent Neural Network,when unrolled through time,can be visualised as-</p>

<p><img src="https://github.com/jasdeep06/jasdeep06.github.io/blob/master/posts/Understanding-LSTM-in-Tensorflow-MNIST/images/Unrolled_RNN.png?raw=True" alt="unrolled_rnn" title=""></p>

<p>Here,</p>

<ol>
<li>x<sub>t</sub> refers to the input at time step t.</li>
<li>s<sub>t</sub> refers to the hidden state at time step t.It can be visualised as “memory” of our network.</li>
<li>o<sub>t</sub> refers to the output at time step t.</li>
<li>U,V and W are parameters that are shared across all the time steps.The significance of this parameter sharing is that our model performs same task at each time step with different input.</li>
</ol>

<p>What we have achieved by unrolling the RNN,is that at each time step,the network can be visualised as feed forward network taking into account the output of the previous time step(signified by the connections between the time steps).</p>



<h3 id="two-caveats">Two caveats</h3>

<p>Our implementation will hinge upon two main concepts which will make us comfortable with our implementation:</p>

<ol>
<li>Interpretation of LSTM cells in tensorflow.</li>
<li>Formatting inputs before feeding them to tensorflow RNNs.</li>
</ol>



<h4 id="interpretation-of-lstm-cells-in-tensorflow">Interpretation of LSTM cells in tensorflow</h4>

<p>A basic LSTM cell is declared in tensorflow as-</p>

<pre class="prettyprint"><code>tf.contrib.rnn.BasicLSTMCell(num_units)
</code></pre>

<p>here <code>num_units</code> refers to the number of units in LSTM cell.</p>

<p><code>num_units</code> can be interpreted as the analogy of hidden layer from the feed forward neural network.The number of nodes in hidden layer of a feed forward neural network is equivalent to <code>num_units</code> number of LSTM units in a LSTM cell at every time step of the network.Following picture should clear any confusion-</p>

<p><img src="https://github.com/jasdeep06/jasdeep06.github.io/blob/master/posts/Understanding-LSTM-in-Tensorflow-MNIST/images/num_units.png?raw=True" alt="num_units" title=""></p>

<p>Each of the <code>num_units</code> LSTM unit can be seen as a standard LSTM unit-</p>

<p><img src="https://github.com/jasdeep06/jasdeep06.github.io/blob/master/posts/Understanding-LSTM-in-Tensorflow-MNIST/images/lstm_unit.png?raw=True" alt="unit" title=""></p>

<p>The above diagram is taken from <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">this incredible blogpost</a> which describes the concept of LSTM effectively.</p>



<h4 id="formatting-inputs-before-feeding-them-to-tensorflow-rnns">Formatting inputs before feeding them to tensorflow RNNs</h4>

<p>The simplest form of RNN in tensorflow is <code>static_rnn</code>.It is defined in tensorflow as </p>

<pre class="prettyprint"><code>tf.static_rnn(cell,inputs)
</code></pre>

<p>There are other arguments as well but we’ll limit ourselves to deal with only these two arguments.</p>

<p>The <code>inputs</code> argument accepts list of tensors of shape <code>[batch_size,input_size]</code>.The length of this list is the number of time steps through which network is unrolled i.e. each element of this list corresponds to the input at respective time step of our unrolled network.</p>

<p>For our case of MNIST images,we have images of size 28X28.They can be inferred as images having 28 rows of 28 pixels each.We will unroll our network through 28 time steps so that at every time step we can input one row of 28 pixels(<code>input_size</code>) and thus a full image through 28 time steps.If we supply <code>batch_size</code> number of images,every time step will be supplied with respective row of <code>batch_size</code> images.Following figure should clear any doubts-</p>

<p><img src="https://github.com/jasdeep06/jasdeep06.github.io/blob/master/posts/Understanding-LSTM-in-Tensorflow-MNIST/images/inputs.png?raw=True" alt="inputs" title=""></p>

<p>The output generated by <code>static_rnn</code> is a list of tensors of shape <code>[batch_size,num_units]</code>.The length of the list is number of time steps through which network is unrolled i.e. one output tensor for each time step.In this implementation we will only be concerned with output of the final time step as the prediction will be generated when all the rows of an image are supplied to RNN i.e. at the last time step.</p>

<p>Now that we have done all the heavy-lifting,we are ready to write the code.The coding part is very straight forward once above concepts are clear.</p>



<h3 id="code">Code</h3>

<p>To start with,lets import necessary dependencies,dataset and declare some constants.We will use <code>batch_size=128</code> and <code>num_units=128</code>.</p>

<pre class="prettyprint"><code>import tensorflow as tf
from tensorflow.contrib import rnn

#import mnist dataset
from tensorflow.examples.tutorials.mnist import input_data
mnist=input_data.read_data_sets("/tmp/data/",one_hot=True)

#define constants
#unrolled through 28 time steps
time_steps=28
#hidden LSTM units
num_units=128
#rows of 28 pixels
n_input=28
#learning rate for adam
learning_rate=0.001
#mnist is meant to be classified in 10 classes(0-9).
n_classes=10
#size of batch
batch_size=128
</code></pre>

<p>Lets now declare placeholders and weights and bias variables which will be used to convert the output of shape <code>[batch_size,num_units]</code> to <code>[batch_size,n_classes]</code> so that correct class can be predicted.</p>

<pre class="prettyprint"><code>#weights and biases of appropriate shape to accomplish above task
out_weights=tf.Variable(tf.random_normal([num_units,n_classes]))
out_bias=tf.Variable(tf.random_normal([n_classes]))

#defining placeholders
#input image placeholder
x=tf.placeholder("float",[None,time_steps,n_input])
#input label placeholder
y=tf.placeholder("float",[None,n_classes])
</code></pre>

<p>Now that we are receiving inputs of shape <code>[batch_size,time_steps,n_input]</code>,we need to convert it into a list of tensors of shape <code>[batch_size,n_inputs]</code> of length <code>time_steps</code> so that it can be then fed to <code>static_rnn</code>.</p>

<pre class="prettyprint"><code>#processing the input tensor from [batch_size,n_steps,n_input] to "time_steps" number of [batch_size,n_input] tensors
input=tf.unstack(x ,time_steps,1)
</code></pre>

<p>Now we are ready to define our network.We will use one layer of BasicLSTMCell and make our <code>static_rnn</code> network out of it.</p>

<pre class="prettyprint"><code>#defining the network
lstm_layer=rnn.BasicLSTMCell(num_units,forget_bias=1)
outputs,_=rnn.static_rnn(lstm_layer,input,dtype="float32")
</code></pre>

<p>As we are considered only with input of last time step,we will generate our prediction out of it.</p>

<pre class="prettyprint"><code>#converting last output of dimension [batch_size,num_units] to [batch_size,n_classes] by out_weight multiplication
prediction=tf.matmul(outputs[-1],out_weights)+out_bias
</code></pre>

<p>Defining loss,optimizer and accuracy.</p>

<pre class="prettyprint"><code>#loss_function
loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))
#optimization
opt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)

#model evaluation
correct_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))
accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))
</code></pre>

<p>Now that we have defined out graph,we can run it.</p>

<pre class="prettyprint"><code>#initialize variables
init=tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init)
    iter=1
    while iter&lt;800:
        batch_x,batch_y=mnist.train.next_batch(batch_size=batch_size)

        batch_x=batch_x.reshape((batch_size,time_steps,n_input))

        sess.run(opt, feed_dict={x: batch_x, y: batch_y})

        if iter %10==0:
            acc=sess.run(accuracy,feed_dict={x:batch_x,y:batch_y})
            los=sess.run(loss,feed_dict={x:batch_x,y:batch_y})
            print("For iter ",iter)
            print("Accuracy ",acc)
            print("Loss ",los)
            print("__________________")

        iter=iter+1
</code></pre>

<p>One crucial thing to note here is that our images were essentially flattened into a single vector of dimensionality 784 to begin with.The function <code>next_batch(batch_size)</code> necessarily returns <code>batch_size</code> batches of these 784 dimensional vectors.They are thus reshaped to <code>[batch_size,time_steps,n_input]</code> so that it can be accepted by our placeholder.</p>

<p>We can also calculate test accuracy of our model-</p>

<pre class="prettyprint"><code>#calculating test accuracy
test_data = mnist.test.images[:128].reshape((-1, time_steps, n_input))
test_label = mnist.test.labels[:128]
print("Testing Accuracy:", sess.run(accuracy, feed_dict={x: test_data, y: test_label}))
</code></pre>

<p>On running,the model runs with a test accuracy of 99.21%.</p>

<p>This blogpost was aimed at making the reader comfortable with the implementational details of RNNs in tensorflow.We’ll built some more complex models to use RNNs effectively in tensorflow.Stay tuned!</p>


<br><p style="text-align:center">Posted on 10 September,2017</p>

      </section>
      <div id="disqus_thread"></div>
    <script type="text/javascript">
          /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
          var disqus_shortname = 'jasdeep06-1'; // required: replace example with your forum shortname
          
          var disqus_url = 'https://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/';
          /* * * DON'T EDIT BELOW THIS LINE * * */
          (function() {
              var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
              dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
              (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
          })();
      </script>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        
        <p class="copyright">Tensorflow maintained by <a href="https://github.com/jasdeep06">jasdeep06</a></p>
        
      </footer>
</div>
</body>
